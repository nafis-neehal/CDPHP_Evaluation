{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Some Notes (Ignore) </H3>\n",
    "<p> Every Python variable as a pointer to an object. When we pass a variable to a function, the variable (pointer) within the function is always a copy of the variable (pointer) that was passed in. So if we assign something new to the internal variable, all we are doing is changing the local variable to point to a different object. This doesn't alter (mutate) the original object that the variable pointed to, nor does it make the external variable point to the new object. At this point, the external variable still points to the original object, but the internal variable points to a new object. If we want to alter the original object (only possible with mutable data types), we have to do something that alters the object without assigning a completely new value to the local variable.</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Init </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [],
   "source": [
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "#ignore warnings.\n",
    "def warn(*args, **kwargs):\n",
    "    pass\n",
    "import warnings\n",
    "warnings.warn = warn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Imports and Constants </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import os, sys, itertools as it, random\n",
    "import datetime as dt\n",
    "import csv\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from pathlib import Path\n",
    "from datetime import datetime\n",
    "from sklearn.metrics import precision_score, accuracy_score, recall_score, balanced_accuracy_score\n",
    "from sklearn.metrics import f1_score, roc_auc_score, log_loss, roc_curve, brier_score_loss, confusion_matrix\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> File Configurations </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_configurations():\n",
    "    with open('config.yaml') as f:\n",
    "        config = yaml.load(f, Loader=yaml.FullLoader)\n",
    "        c_gen = config['c_gen']\n",
    "        c_p = config['c_p']\n",
    "        c_r = config['c_r']\n",
    "        c_e = config['c_e']\n",
    "        c_visual = config['c_visual']\n",
    "        \n",
    "        #preprocessing\n",
    "        c_e['eval_date'] = pd.to_datetime(c_e['eval_date'])\n",
    "    \n",
    "    return c_gen, c_p, c_r, c_e, c_visual"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> HELPER FUNCTIONS </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for generation\n",
    "#generate random date between a range\n",
    "#input in datetime.date format\n",
    "#mutable obj integrity checked\n",
    "def generate_random_date(start_date, end_date, iteration, date_format):\n",
    "    date_list = []\n",
    "    seed = 0\n",
    "    for i in range(iteration):\n",
    "        s_date = datetime.strptime(start_date, '%Y-%m-%d')\n",
    "        e_date = datetime.strptime(end_date, '%Y-%m-%d')\n",
    "        time_between_dates = e_date - s_date\n",
    "        days_between_dates = time_between_dates.days\n",
    "        random.seed(seed)\n",
    "        random_number_of_days = random.randrange(days_between_dates)\n",
    "        random_date = s_date + dt.timedelta(days=random_number_of_days) #this is datetime object\n",
    "        if date_format=='y-m-d':\n",
    "            random_date = random_date.strftime('%Y-%m-%d') #string format output\n",
    "        elif date_format=='ym':\n",
    "            random_date = int(random_date.strftime(\"%Y%m\"))\n",
    "        date_list.append(random_date)\n",
    "        seed += 1\n",
    "    \n",
    "    return date_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for evaluation\n",
    "#preprocess referral dates\n",
    "#convert any day in any month to the first day of that month in REFERRAL data\n",
    "#mutable obj integrity checked\n",
    "#ref_copy passed by reference, changed directly\n",
    "def convert_dates_ref(ref_copy, c_r): #use if c_r['date_format'] is 'y-m-d' or datetime\n",
    "    ref_date_column = ref_copy[c_r['columns']['date_column']]\n",
    "    for i in range(len(ref_date_column)):\n",
    "        formatted_date = ref_date_column[i]\n",
    "        new_formatted_date = datetime(formatted_date.year, formatted_date.month, c_r['day_to_evaluate'])\n",
    "        ref_copy.at[i,c_r['columns']['date_column']] = new_formatted_date\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for evaluation\n",
    "#convert top k probability of a target to 1, O/W to 0 mapping\n",
    "#target was a pd.series, kept intact\n",
    "#mutable obj integrity checked\n",
    "def prob_to_bin(target, k):\n",
    "    target_bin = target.copy()\n",
    "    target_bin[target_bin.index] = 0\n",
    "    idx = target.nlargest(k).index\n",
    "    target_bin[idx] = 1\n",
    "\n",
    "    return target_bin\n",
    "#return local variable, it does not get lost, stays in heap as long as there is a reference to it. O/W garbage collector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for evaluation\n",
    "'''\n",
    "pd.to_Datetime(anything) = create datetime object\n",
    "datetime.strptime(string) -> datetime object\n",
    "datetime.strftime(datetime) -> string in different formats\n",
    "'''\n",
    "#give a 'ym' column, it will convert all of them to datetime\n",
    "#mutable obj integrity checked\n",
    "#updated mutable dataframe passed by reference\n",
    "def ym_to_datetime(config, df): #ym to datetime\n",
    "    label = config['columns']['date_column']\n",
    "    df[label] = df[label].astype(str)\n",
    "    for i in range(df[label].size):\n",
    "        df[label][i] = df[label][i][:4] + '-' + df[label][i][4:] + '-01' #choosing the first month of the day\n",
    "    df[label] = pd.to_datetime(df[label])\n",
    "    #return df\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "#for evaluation\n",
    "def drop_recent_referrals(c_p, c_r, c_e, ref_copy, pred_copy):\n",
    "    start = c_e['eval_date'] - pd.DateOffset(months=c_e['recent_ref_window'])\n",
    "    end = c_e['eval_date'] - pd.DateOffset(months=1) #ends the previous month of the current month\n",
    "    mask = (ref_copy[c_r['columns']['date_column']]>=start) & (ref_copy[c_r['columns']['date_column']]<=end)\n",
    "    \n",
    "    #those IDs from prediction (0-500) that are not in list of those IDs that fall in daterange of all IDs(0-1000)\n",
    "    idx = ~pred_copy[c_p['columns']['id_column']].isin(ref_copy.loc[mask][c_r['columns']['id_column']])\n",
    "    pred_copy = pred_copy[idx]\n",
    "    return pred_copy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "#[0,3] -> datetime range in string\n",
    "def window_to_range(c_e, window):\n",
    "    \n",
    "    if(window[0]<0):\n",
    "        start = c_e['eval_date'] - pd.DateOffset(months=(-1)*window[0])\n",
    "    else:\n",
    "        start = c_e['eval_date'] + pd.DateOffset(months=window[0])\n",
    "\n",
    "    end = c_e['eval_date'] + pd.DateOffset(months=window[1])\n",
    "    \n",
    "    start = start.strftime('%Y/%m')\n",
    "    end = end.strftime('%Y/%m')\n",
    "    \n",
    "    win_range = start + '-' + end\n",
    "    return win_range"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Generate Synthetic Data </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for generaiton\n",
    "#mutable obj integrity checked\n",
    "#no changes in c_p, c_r\n",
    "def generate_synthetic_ground_truth_data(c_gen, c_r):\n",
    "    seed = 1234\n",
    "    np.random.seed(seed)\n",
    "    patients = pd.Series([x for x in np.random.randint(0, c_gen['ref']['upper_bound'], c_gen['ref']['num_samples'])])\n",
    "    patients = patients.sort_values(ascending=True).reset_index(drop=True)\n",
    "    date = pd.Series([d for d in generate_random_date(c_gen['ref']['start_date'], c_gen['ref']['end_date'], c_gen['ref']['num_samples'], c_r['date_format'])])\n",
    "    data = pd.DataFrame({c_r['columns']['id_column']:patients, c_r['columns']['date_column']:date})\n",
    "    data.to_csv(c_r['dir']+c_r['file'], index = False, float_format= '%8.5f', mode='w')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for generation\n",
    "#mutable obj integrity checked\n",
    "#no changes in c_p\n",
    "def generate_synthetic_prediction_data(c_gen, c_p):\n",
    "    patients = pd.Series(range(0, c_gen['pred']['num_samples'])) #500\n",
    "    \n",
    "    if c_p['date_format']=='ym':\n",
    "        date = pd.date_range(c_gen['pred']['start_date'], c_gen['pred']['end_date'], freq='MS').strftime(\"%Y%m\").astype(int) #25\n",
    "    elif c_p['date_format']=='y-m-d':\n",
    "        date = pd.date_range(c_gen['pred']['start_date'], c_gen['pred']['end_date'], freq='MS').strftime(\"%Y-%m-%d\")\n",
    "    else:\n",
    "        date = pd.date_range(c_gen['pred']['start_date'], c_gen['pred']['end_date'], freq='MS')\n",
    "    \n",
    "    data = pd.DataFrame(list(it.product(patients,date)),columns=[c_p['columns']['id_column'],c_p['columns']['date_column']])\n",
    "    seed = 1234\n",
    "    for model in c_gen['pred']['model_columns']:\n",
    "        np.random.seed(seed)\n",
    "        data[model] = pd.Series(np.random.random((data.shape[0])))\n",
    "        seed+=1\n",
    "    \n",
    "    #dataframe_to_csv(data, c_p['dir']+c_p['file'])\n",
    "    data.to_csv(c_p['dir']+c_p['file'], index=False, float_format= '%8.5f', mode='w')\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Score Class </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#for evaluation\n",
    "class Score:\n",
    "    \n",
    "    def __init__(self, model_name):\n",
    "        self.model_name = model_name\n",
    "        self.experimental_samples = -1\n",
    "        self.precision = {}\n",
    "        self.recall = {}\n",
    "        self.accuracy = {}\n",
    "        self.balanced_acc = {}\n",
    "        self.f1_score = {}\n",
    "        self.roc_auc_score = {} \n",
    "        self.log_loss = {}\n",
    "        self.brier_score_loss = {}\n",
    "        self.confusion_matrix = {}\n",
    "    \n",
    "    def get_precision(self, y_true, y_predict, average='binary'):\n",
    "        return precision_score(y_true, y_predict, average)\n",
    "    \n",
    "    def get_recall(self, y_true, y_predict, average='binary'):\n",
    "        return recall_score(y_true, y_predict, average)\n",
    "    \n",
    "    def get_accuracy(self, y_true, y_predict):\n",
    "        return recall_score(y_true, y_predict)\n",
    "    \n",
    "    def get_balanced_acc(self, y_true, y_predict):\n",
    "        return balanced_accuracy_score(y_true, y_predict)\n",
    "    \n",
    "    def get_f1_score(self, y_true, y_predict, average='binary'):\n",
    "        return f1_score(y_true, y_predict, average='binary')\n",
    "    \n",
    "    def get_roc_auc_score(self, y_true, y_predict):\n",
    "        return roc_auc_score(y_true, y_predict)\n",
    "    \n",
    "    def get_log_loss(self, y_true, y_predict):\n",
    "        return log_loss(y_true, y_predict)\n",
    "\n",
    "    def get_brier_score_loss(self, y_true, y_predict):\n",
    "        return brier_score_loss(y_true, y_predict)\n",
    "    \n",
    "    def get_confusion_matrix(self, y_true, y_predict):\n",
    "        return confusion_matrix(y_true, y_predict)\n",
    "    \n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Evaluation </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#cp, c_r, c_e are all mutable\n",
    "#mutable obj integrity checked\n",
    "def evaluate(c_p, c_r, c_e):\n",
    "    \n",
    "    eval_method = c_e['eval_method']      \n",
    "    \n",
    "    referral = pd.read_csv(c_r['dir'] + c_r['file'])\n",
    "    ref_copy = referral.copy() #shallow copy\n",
    "    \n",
    "    #convert_dates_ref(c_r)\n",
    "    if c_r['date_format']=='ym':\n",
    "        ym_to_datetime(c_r, ref_copy) #passing reference of mutable ref_copy, will be directly edited\n",
    "    else:\n",
    "        ref_copy[c_r['columns']['date_column']] = pd.to_datetime(ref_copy[c_r['columns']['date_column']])\n",
    "        convert_dates_ref(ref_copy, c_r)\n",
    "    \n",
    "    #now both pred_copy['Date'] and ref_copy['Date'] is in Datetime format instead of previous String format\n",
    "    \n",
    "    prediction = pd.read_csv(c_p['dir'] + c_p['file'])\n",
    "    pred_copy = prediction.copy()\n",
    "        \n",
    "    #change date from ym to ymd here\n",
    "    if c_p['date_format']=='ym':\n",
    "        ym_to_datetime(c_p, pred_copy) #passing reference of mutable pred_copy, will be directly edited\n",
    "    else:\n",
    "        pred_copy[c_p['columns']['date_column']] = pd.to_datetime(pred_copy[c_p['columns']['date_column']])\n",
    "        \n",
    "    #now both ref_copy and pred_copy are datetime\n",
    "    ##only applying top_k in selected range, not the whole column-------------------->>>>>>>\n",
    "    pred_copy = pred_copy.loc[pred_copy[c_p['columns']['date_column']]==c_e['eval_date']] \n",
    "    \n",
    "    all_model_list = c_e['eval_models']\n",
    "    \n",
    "    #if all models predicts -1, then I can safely drop patient-month-date row with -1 predictions here\n",
    "    #select rows, where the first model is not -1 (all model predicts -1)\n",
    "    if c_e['drop_neg_prob'] == True:\n",
    "        pred_copy = pred_copy[pred_copy[all_model_list[0]]!=-1]\n",
    "    \n",
    "    #remove recent referrals checking back k months in referral and drop them from prediction\n",
    "    if c_e['drop_ref'] == True:\n",
    "        pred_copy = drop_recent_referrals(c_p, c_r, c_e, ref_copy, pred_copy)\n",
    "    \n",
    "    #pivot referral table\n",
    "    ref_copy['target'] = pd.Series(np.ones(ref_copy.shape[0], dtype=float))\n",
    "    ref_copy = ref_copy.pivot_table(index=c_r['columns']['id_column'], columns=c_r['columns']['date_column'], values='target', aggfunc='sum')\n",
    "    ref_copy = ref_copy.fillna(0)\n",
    "    \n",
    "    all_model_evaluations = {} #{'model_name':score class object for that model}\n",
    "        \n",
    "    #now branch out for each model\n",
    "    for model in all_model_list:\n",
    "        \n",
    "        evaluated_model_obj = Score(model)\n",
    "        \n",
    "        #calculate for each window\n",
    "        for window in c_e['eval_windows']:\n",
    "            \n",
    "            #taking negative window into consideration\n",
    "            if(window[0]<0):\n",
    "                start = c_e['eval_date'] - pd.DateOffset(months=(-1)*window[0])\n",
    "            else:\n",
    "                start = c_e['eval_date'] + pd.DateOffset(months=window[0])\n",
    "                \n",
    "            end = c_e['eval_date'] + pd.DateOffset(months=window[1])\n",
    "            \n",
    "            sl=slice(start,end)\n",
    "            y_true = ref_copy.loc[:,sl] #also a shallow copy\n",
    "            \n",
    "            #aggregate referrals\n",
    "            y_true = y_true.sum(axis=1)\n",
    "            \n",
    "            #aggregated referral map to 1 if >1\n",
    "            y_true[y_true>1] = 1\n",
    "            \n",
    "            #for the time being, all ref patients are in pred_list. \n",
    "            #Change random.randint upper_range in data_generate to tweak\n",
    "            y_true=y_true[y_true.index.isin(pred_copy[c_p['columns']['id_column']])]\n",
    "            \n",
    "            y_true=y_true.append(pd.Series(0,index=set(pred_copy[c_p['columns']['id_column']])-set(y_true.index))).sort_index()\n",
    "        \n",
    "            #their size have to be same -- TO USE the built in metric functions\n",
    "            if pred_copy.shape[0]!=len(y_true):\n",
    "                print(\"ERROR: PREDICTION AND ACTUAL DATAFRAME HAVE DIFFERENT NUMBERS.  PREDICTION:\",\n",
    "                      pred_copy.shape[0], \" EVALUATION: \",len(y_true))\n",
    "                break\n",
    "            \n",
    "            #now thresholding method\n",
    "            if eval_method=='top_k':\n",
    "                for k_values in c_e['top_k']:\n",
    "                    label = model+'_window_['+str(window[0])+','+str(window[1])+']_'+eval_method + '_@' + str(k_values)\n",
    "                    pred_copy[label] = prob_to_bin(pred_copy[model], k_values)\n",
    "                    #model score for this (window,k) update\n",
    "                    update_model_score(model, evaluated_model_obj, label, y_true, pred_copy)\n",
    "    \n",
    "            elif eval_method == 'thresholding':\n",
    "                for thresholds in c_e['thresholding']:\n",
    "                    label = model+'_window_['+str(window[0])+','+str(window[1])+']_'+eval_method + '_@' + str(thresholds)\n",
    "                    pred_copy[label] = np.where(pred_copy[model] > thresholds, 1, 0)\n",
    "                    #model score for this (window,threshold) update\n",
    "                    update_model_score(model, evaluated_model_obj, label, y_true, pred_copy)\n",
    "            \n",
    "        all_model_evaluations.update({model:evaluated_model_obj})\n",
    "    \n",
    "    return all_model_evaluations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#mutable obj integrity checked\n",
    "#evaluated_model_obj changed as requirement\n",
    "def update_model_score(model, evaluated_model_obj, label, y_true, pred_copy):\n",
    "    \n",
    "    precision = evaluated_model_obj.get_precision(y_true.values, pred_copy[label].values)\n",
    "    evaluated_model_obj.precision.update({label:precision})\n",
    "\n",
    "    recall = evaluated_model_obj.get_recall(y_true.values, pred_copy[label].values)\n",
    "    evaluated_model_obj.recall.update({label:recall})\n",
    "\n",
    "    accuracy = evaluated_model_obj.get_accuracy(y_true.values, pred_copy[label].values)\n",
    "    evaluated_model_obj.accuracy.update({label:accuracy})\n",
    "\n",
    "    balanced_acc = evaluated_model_obj.get_balanced_acc(y_true.values, pred_copy[label].values)\n",
    "    evaluated_model_obj.balanced_acc.update({label:balanced_acc})\n",
    "\n",
    "    f1_score = evaluated_model_obj.get_f1_score(y_true.values, pred_copy[label].values)\n",
    "    evaluated_model_obj.f1_score.update({label:f1_score})\n",
    "    \n",
    "    confusion_matrix = evaluated_model_obj.get_confusion_matrix(y_true.values, pred_copy[label].values)\n",
    "    evaluated_model_obj.confusion_matrix.update({label:confusion_matrix})\n",
    "\n",
    "    #these will take predicted probabilites, not thresholded binaries\n",
    "\n",
    "    log_loss = evaluated_model_obj.get_log_loss(y_true.values, pred_copy[model].values)\n",
    "    evaluated_model_obj.log_loss.update({label:log_loss})\n",
    "\n",
    "    roc_auc_score = evaluated_model_obj.get_roc_auc_score(y_true.values, pred_copy[model].values)\n",
    "    evaluated_model_obj.roc_auc_score.update({label:roc_auc_score})\n",
    "    \n",
    "    brier_score_loss = evaluated_model_obj.get_brier_score_loss(y_true.values, pred_copy[model].values)\n",
    "    evaluated_model_obj.brier_score_loss.update({label:brier_score_loss})\n",
    "    \n",
    "    #add number of samples used by this model\n",
    "    evaluated_model_obj.experimental_samples = y_true.shape[0]\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H3> Present Evaluation </H3>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {
    "code_folding": []
   },
   "outputs": [],
   "source": [
    "#generate plottable dataframe from whole result: evaluation_to_dataframe() helper function\n",
    "#visualize result (if visual==True): visualize_performance() helper_function\n",
    "#save result to CSV file: Use dataframe_to_csv() helper function\n",
    "#mutable obj integrity checked\n",
    "def present_evaluation(c_p, c_r, c_e, c_visual, all_model_evaluations, table=False, plot = False, save=False):\n",
    "    \n",
    "    #result file\n",
    "    result_file = c_e['dir'] + c_e['file']\n",
    "    \n",
    "    #clear if there is already any result file previously (from previous run/experiment), \n",
    "    #otherwise it will just keep appending, as opening in append mode\n",
    "    if os.path.exists(result_file):\n",
    "        os.remove(result_file)\n",
    "        \n",
    "    data = generate_tabular_data(c_p, c_r, c_e, all_model_evaluations)\n",
    "    \n",
    "    if c_visual['table']['show'] == True:\n",
    "        pd.set_option('display.max_columns', 9999)\n",
    "        pd.options.display.float_format = '{:8.5f}'.format\n",
    "        display(data)\n",
    "        \n",
    "    #will add graph here\n",
    "    if c_visual['model_comparison']['plot'] == True:\n",
    "        generate_comparison_plot(c_e, data, c_visual['model_comparison']['windows'], c_visual['model_comparison']['metrics'])\n",
    "    \n",
    "    if c_visual['confusion_matrix']['plot'] == True:\n",
    "        generate_confusion_matrix_plot(c_e, all_model_evaluations, c_visual['confusion_matrix']['model'], c_visual['confusion_matrix']['window'], \n",
    "                                       c_visual['confusion_matrix']['thres'])\n",
    "    \n",
    "    if c_visual['probability_distribution']['plot'] == True:\n",
    "        generate_probability_distribution_plot(c_p, c_visual['probability_distribution']['models'])\n",
    "    \n",
    "    if c_e['save_csv'] == True:\n",
    "        data.to_csv(result_file, index = False, float_format= '%8.5f', mode='w')\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "#name of models, windows, top_ks\n",
    "def generate_confusion_matrix_plot(c_e, all_model_evaluations, model, window, thres):\n",
    "    confusion_matrix = all_model_evaluations[model].confusion_matrix\n",
    "    win = '[' + str(window[0]) + ',' + str(window[1]) + ']'\n",
    "    th = str(thres)\n",
    "    for keys in confusion_matrix.keys():\n",
    "        if win in keys and th in keys:\n",
    "            conf_mat = confusion_matrix[keys]\n",
    "            conf_mat = pd.DataFrame(conf_mat)\n",
    "            akws = {\"ha\": 'left',\"va\": 'top'}\n",
    "            plt.figure()\n",
    "            ax = sns.heatmap(conf_mat, annot=True, fmt=\"d\", annot_kws=akws, cmap='Greens_r')\n",
    "            ax.set_title(\"Confusion Matrix of \" + model + ' for window ' + win + ' for ' + c_e['eval_method'] + '_@' + str(thres))\n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "#input: c_p, list_of_models\n",
    "#output: probability distribution plots\n",
    "def generate_probability_distribution_plot(c_p, models):\n",
    "    file = c_p['dir'] + c_p['file']\n",
    "    pred = pd.read_csv(file)\n",
    "    for each_model in models:\n",
    "        plt.figure()\n",
    "        ax = sns.distplot(pred[each_model])\n",
    "        ax.set_title(\"Probability Distribution of \" + each_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_comparison_plot(c_e, data, plot_window, metrics):\n",
    "    \n",
    "    for window in plot_window:\n",
    "        \n",
    "        win = window_to_range(c_e, window)\n",
    "        new_data = data[data['Window']==win]\n",
    "\n",
    "        for t in c_e[c_e['eval_method']]:\n",
    "            metric_col = [col for col in new_data.columns if str(t) in col] #original column names for metrics (All 8 metric columns)\n",
    "            final_original_columns = ['Model']\n",
    "            final_modified_columns = ['Model']\n",
    "\n",
    "            for metric_name in metrics:\n",
    "                final_original_columns = final_original_columns + [col for col in metric_col if metric_name in col] #only columns passed\n",
    "                final_modified_columns = final_modified_columns + [metric_name for col in metric_col if metric_name in col]\n",
    "\n",
    "            pf = new_data[final_original_columns].copy()\n",
    "\n",
    "            for i in range(len(pf.columns)):\n",
    "                pf.rename(columns={final_original_columns[i]:final_modified_columns[i]}, inplace=True)\n",
    "\n",
    "            #pivot the data to covert wide dataframe to long dataframe\n",
    "            pf_new = pd.melt(pf, id_vars=['Model'], value_vars=final_modified_columns[1:])\n",
    "\n",
    "            data_plot = sns.catplot(x=\"variable\", y=\"value\", hue=\"Model\", data=pf_new, kind=\"bar\", height = 4, aspect = 3)\n",
    "            data_plot.set_xlabels(\"Metrics\")\n",
    "            data_plot.set_ylabels(\"Score\")\n",
    "            title = \"Performance of Models During \" + win + \" with \" + c_e['eval_method'] + \"_@\" + str(t)\n",
    "            data_plot.fig.suptitle(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#takes all info, returns dataframe \n",
    "def generate_tabular_data(c_p, c_r, c_e, all_model_evaluations):\n",
    "    \n",
    "    data = pd.DataFrame()\n",
    "    row = 0\n",
    "    metric_name = c_visual['table']['metrics']\n",
    "    #conf_mat = {\"TN\":0, \"FP\":0, \"FN\":0, \"TP\":0}\n",
    "    conf_head = [\"TN\", \"FP\", \"FN\", \"TP\"]\n",
    "    \n",
    "    for model in all_model_evaluations:\n",
    "        for window in c_e['eval_windows']:\n",
    "            sub_key = '['+str(window[0])+','+str(window[1])+']'\n",
    "            data.loc[row, 'Start Time'] = pd.Timestamp.now(tz='US/Eastern')\n",
    "            data.loc[row, 'Experiment Name'] = c_e['experiment']\n",
    "            data.loc[row, 'Model'] = model\n",
    "            data.loc[row, 'Window'] = window_to_range(c_e, window)\n",
    "            data.loc[row, 'Eval Date'] = c_e['eval_date']\n",
    "            data.loc[row, 'Num Samples'] = all_model_evaluations[model].experimental_samples\n",
    "            \n",
    "            #-- change here if both k and thresh needed\n",
    "            for metric in metric_name:\n",
    "                \n",
    "                values  = [value for key, value in all_model_evaluations[model].__dict__[metric].items() if sub_key in key] #[K_50, K_60..]\n",
    "                    \n",
    "                if c_e['eval_method'] == 'top_k':\n",
    "                    \n",
    "                    for i in range(len(c_e['top_k'])):\n",
    "                        if metric=='confusion_matrix':\n",
    "                            conf_mat = values[i].ravel()\n",
    "                            \n",
    "                            for j in range(len(conf_mat)):\n",
    "                                label = conf_head[j]\n",
    "                                col_label = label + '_@k='+str(c_e['top_k'][i])\n",
    "                                data.ix[row, col_label] = conf_mat[j] \n",
    "                            \n",
    "                        else:\n",
    "                            col_label = metric + '_@k='+str(c_e['top_k'][i])\n",
    "                            data.loc[row, col_label] = values[i]\n",
    "                \n",
    "                elif c_e['eval_method'] == 'thresholding':\n",
    "                    for i in range(len(c_e['thresholding'])):\n",
    "                        col_label = metric + '_p>'+str(c_e['thresholding'][i])\n",
    "                        data.loc[row, col_label] = values[i]\n",
    "            \n",
    "            #source files\n",
    "            data.loc[row, 'Prediction Source'] = c_p['dir'] + c_p['file']\n",
    "            data.loc[row, 'Referral Source'] = c_r['dir'] + c_r['file']\n",
    "            data.loc[row, 'Result Output'] = c_e['dir'] + c_e['file']\n",
    "            \n",
    "            #timestamps\n",
    "            data.loc[row, 'End Time'] = pd.Timestamp.now(tz='US/Eastern')\n",
    "            data.loc[row, 'Total Time'] = data.loc[row, 'End Time'] - data.loc[row, 'Start Time']\n",
    "            row = row + 1\n",
    "    \n",
    "    data['Num Samples'] = data['Num Samples'].astype(int)\n",
    "\n",
    "    #moving the start time col to end\n",
    "    cols = data.columns.tolist()\n",
    "    cols = cols[1:-2] + [cols[0]] + cols[-2:]\n",
    "    data = data[cols]\n",
    "    \n",
    "    return data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<H2> MAIN </H2>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'config.yaml'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-122-e9a983745bb4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mset_option\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'display.max_rows'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;31m#to show all the rows of a dataframe\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0m__name__\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;34m\"__main__\"\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 3\u001b[1;33m     \u001b[0mc_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_p\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_r\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_e\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_visual\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mload_configurations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      4\u001b[0m     \u001b[0mgenerate_synthetic_ground_truth_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_r\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m     \u001b[0mgenerate_synthetic_prediction_data\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mc_gen\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mc_p\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m<ipython-input-105-ab27ed98d0ee>\u001b[0m in \u001b[0;36mload_configurations\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mload_configurations\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m     \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'config.yaml'\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m         \u001b[0mconfig\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mLoader\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0myaml\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mFullLoader\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m         \u001b[0mc_gen\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c_gen'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m         \u001b[0mc_p\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mconfig\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'c_p'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'config.yaml'"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', None) #to show all the rows of a dataframe\n",
    "if __name__ == \"__main__\":\n",
    "    c_gen, c_p, c_r, c_e, c_visual = load_configurations()\n",
    "    generate_synthetic_ground_truth_data(c_gen, c_r)\n",
    "    generate_synthetic_prediction_data(c_gen, c_p)\n",
    "    all_model_evaluations = evaluate(c_p, c_r, c_e) #neg window can be controlled from config\n",
    "    present_evaluation(c_p, c_r, c_e, c_visual, all_model_evaluations)\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
